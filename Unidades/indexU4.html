<!DOCTYPE html>
<html lang="en">

<head>
  <link rel="preconnect" href="https://fonts.googleapis.com" />
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin />
  <link
    href="https://fonts.googleapis.com/css2?family=Montserrat+Alternates:ital,wght@0,100;0,200;0,300;0,400;0,500;0,600;0,700;0,800;0,900;1,100;1,200;1,300;1,400;1,500;1,600;1,700;1,800;1,900&family=Montserrat:ital,wght@0,100..900;1,100..900&family=Play:wght@400;700&family=Urbanist:ital,wght@0,100..900;1,100..900&display=swap"
    rel="stylesheet" />
  <!--STYLES-->
  <link rel="shortcut icon" href="../img/Imagen de WhatsApp 2023-12-03 a las 16.16.28_23eff904.jpg" type="image/x-icon">

  <link rel="stylesheet" href="../css/styles-default.css" />
  <link rel="stylesheet" href="../css/stylesU4.css" />
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <title>Unidad 4</title>
</head>

<body>
  <!-- CONTENEDOR DE MENU DE NAVEGACION -->
  <div class="container">
    <nav class="nav">
      <h1 class="logo">Arquitectura De Computadoras</h1>
      <ul class="nav-menu">
        <li>
          <a href="../index.html">Inicio</a>
        </li>
        <li>
          <a href="indexu1.html">Unidad 1</a>
        </li>
        <li>
          <a href="indexU2.html">Unidad 2</a>
        </li>
        <li>
          <a href="indexU3.html">Unidad 3</a>
        </li>
        <li>
          <a href="indexU4.html" class="activo">Unidad 4</a>
        </li>
        <li>
          <a href="Practicas.html">Practicas</a>
        </li>
      </ul>
    </nav>
  </div>
  <div class="title">
    <h1>Unidad 4 Aspectos básicos de la computación paralela</h1>
  </div>

  <div class="conte">
    <main>
      <article class="contenidos">
        <h3>
          4.1 Aspectos básicos de la computación paralela
        </h3>
        <p>

          <img src="../img/Paralela.jpg" alt="">
          La computación paralela se basa en la idea de dividir un problema en tareas más pequeñas y procesarlas de
          manera simultánea utilizando múltiples recursos de computación. Esto permite un procesamiento más rápido y
          eficiente en comparación con los enfoques secuenciales tradicionales. Algunos aspectos fundamentales de la
          computación paralela incluyen la sincronización de tareas, la comunicación entre procesos y la gestión de
          recursos.


        </p>
      </article>
      <article class="contenidos">
        <h3>4.2 Tipos de computación paralela</h3>
        <p>
          <img src="../img/paralelaaa.jpg" alt="">
          Existen varios tipos de computación paralela que se utilizan en diferentes contextos y escenarios. Algunos de
          los enfoques más comunes incluyen el procesamiento paralelo a nivel de bit, a nivel de instrucción, a nivel de
          datos y a nivel de tarea. Estos enfoques se diferencian en cómo se dividen y procesan las tareas y los datos.
        </p>
      </article>
      <article class="contenidos">
        <h3>4.2.1 Clasificación</h3>
        <p>
          <img src="../img/papa.jpg" alt="">
          La clasificación de la computación paralela puede realizarse en función de la forma en que se dividen las
          tareas y los datos, así como de la forma en que se coordinan y comunican los procesos paralelos. Algunas
          clasificaciones comunes incluyen la computación paralela a nivel de bit, a nivel de instrucción, a nivel de
          datos y a nivel de tarea.
        </p>

      </article>
      <article class="contenidos">

        <h3>4.2.2 Arquitectura de computadores secuenciales</h3>
        <p>
          <img src="../img/seuc.png" alt="">
          La arquitectura de computadores secuencial se refiere a los sistemas informáticos tradicionales en los que las
          instrucciones se ejecutan una tras otra en secuencia. Este tipo de arquitectura sigue siendo común en muchas
          computadoras personales y estaciones de trabajo.
        </p>
      </article>
      <article class="contenidos">
        <h3>4.2.3 Organización de direcciones de memoria</h3>
        <p>
          <img src="../img/memor.gif" alt="">
          La organización de direcciones de memoria se refiere a cómo se asignan y acceden a las direcciones de memoria
          en un sistema de computación paralela. Esto incluye consideraciones como la memoria compartida, la memoria
          distribuida y las técnicas de direccionamiento utilizadas para acceder a los datos en paralelo.
        </p>
      </article>

      <article class="contenidos">
        <h3>4.3 Sistema de memoria compartida</h3>
        <p>
          <img src="../img/redes.jpeg" alt="">
          Los sistemas de memoria compartida son un enfoque de computación paralela en el que múltiples procesadores
          acceden a una misma área de memoria compartida. Esto permite a los procesadores compartir datos y comunicarse
          de manera eficiente. Dentro de los sistemas de memoria compartida, existen dos tipos principales de redes: las
          redes de medio compartida y las redes conmutadas.
        </p>
      </article>
      <article class="contenidos">
        <h3>4.3.1.1 Redes de medio compartida</h3>
        <p>
          <img src="../img/redes2.jpg" alt="">
          Las redes de medio compartida son un tipo de arquitectura de memoria compartida en la que los procesadores se
          conectan físicamente a un bus compartido o a una red de interconexión. Los procesadores pueden leer y escribir
          en la memoria compartida a través de este medio compartido.
        </p>
      </article>

      <article class="contenidos">
        <h3>4.3.1.2 Redes conmutadas</h3>
        <p>
          <img src="../img/redes3.png" alt="">
          Las redes conmutadas, por otro lado, utilizan interruptores o conmutadores para establecer conexiones entre
          los procesadores y la memoria compartida. Estas redes ofrecen una mayor escalabilidad y capacidad de
          comunicación en comparación con las redes de medio compartida.
        </p>
      </article>



      <article class="contenidos">
        <h3>4.4 Sistemas de memoria construida</h3>
        <p>
          Los sistemas de memoria construida son una forma de organización de la memoria en la computación paralela en
          la que cada procesador tiene su propia memoria local. Esto permite una mayor independencia entre los
          procesadores y reduce la necesidad de acceder a una memoria compartida.
        </p>
      </article>

      <article class="contenidos">
        <h3>4.5 Casos de estudio</h3>
        <p>
          En el campo de la computación paralela, existen numerosos casos de estudio que han demostrado la eficacia y
          los beneficios de los enfoques paralelos en diferentes dominios. Algunos ejemplos incluyen el uso de
          computación paralela en simulaciones científicas, análisis de grandes conjuntos de datos, renderizado de
          gráficos y modelado de sistemas complejos.
        </p>
      </article>
    </main>

  </div>

  <style>
    body {
      background-image: linear-gradient(#34323254, #1b1818aa),
        image-set("../img/compuParalela.webp");

    }
  </style>

</body>

</html>